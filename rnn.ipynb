{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna import Trial, Study\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from utils.training import train, best_torch_device\n",
    "from torchinfo import summary\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = best_torch_device()\n",
    "# device = torch.device('cpu')\n",
    "print(best_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = np.load(\"data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"data/y_train_valid.npy\")\n",
    "person_train_valid = np.load(\"data/person_train_valid.npy\")\n",
    "\n",
    "X_train_aug = np.load(\"data/generated/frequency/X_train_augmented.npy\")\n",
    "y_train_aug = np.load(\"data/generated/frequency/y_train_augmented.npy\")\n",
    "person_train_aug = np.load(\"data/generated/frequency/person_train_augmented.npy\")\n",
    "\n",
    "X_test = np.load(\"data/X_test.npy\")\n",
    "y_test = np.load(\"data/y_test.npy\")\n",
    "person_test = np.load(\"data/person_test.npy\")\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_train_aug -= 769\n",
    "y_test -= 769\n",
    "\n",
    "X_train = X_train_valid[:1777]\n",
    "y_train = y_train_valid[:1777]\n",
    "X_valid = X_train_valid[1777:]\n",
    "y_valid = y_train_valid[1777:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8885, 22, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super(Permute, self).__init__()\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "    \n",
    "def train(model, optimizer, loader, val_loader, cel_loss, num_epochs):\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    for epoch_idx in tqdm(range(num_epochs)):\n",
    "        # Set model to train mode - useful for layers such as BatchNorm or Dropout whose behaviors change between train/eval\n",
    "        model.train()\n",
    "        train_count = 0\n",
    "        train_correct_count = 0\n",
    "        for batch_idx, (train_x, train_y) in enumerate(loader):\n",
    "            train_x = train_x.float().to(device)\n",
    "            train_y = train_y.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(train_x)\n",
    "            loss = cel_loss(logits, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat = torch.argmax(logits, dim=-1)\n",
    "                train_correct_count += torch.sum(y_hat == train_y, axis=-1)\n",
    "                train_count += train_x.size(0)\n",
    "\n",
    "        train_acc = train_correct_count / train_count\n",
    "        train_acc_hist.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_count = 0\n",
    "        val_correct_count = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (val_x, val_y) in enumerate(val_loader):\n",
    "                val_x = val_x.float().to(device)\n",
    "                val_y = val_y.long().to(device)\n",
    "                logits = model(val_x).detach()\n",
    "                y_hat = torch.argmax(logits, dim=-1)\n",
    "                val_correct_count += torch.sum(y_hat == val_y, axis=-1)\n",
    "                val_count += val_x.size(0)\n",
    "        val_acc = val_correct_count / val_count\n",
    "        val_acc_hist.append(val_acc)\n",
    "        print('Train acc: {:.3f}, Val acc: {:.3f}'.format(train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.tdd = nn.Sequential(\n",
    "            nn.Linear(22, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(40)\n",
    "\n",
    "        self.num_layers = 4 # 3, 4\n",
    "        self.hidden_size = 20 # 20\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=40,\n",
    "            hidden_size=self.hidden_size,\n",
    "            dropout=0.5,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(20),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LazyLinear(out_features=4),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)  # the dim corresponds to num_output_classes=4\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x should have shape (N, H, L) = (N, 22, 1000), where\n",
    "            L = sequence length\n",
    "            N = batch size\n",
    "            H = input size\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1) # (N, 1000, 22)\n",
    "        x = self.tdd(x) # (N, 1000, 40)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # device = torch.device('mps')\n",
    "        # h0 = torch.randn(self.num_layers, self.hidden_size).to(device)\n",
    "        # c0 = torch.randn(self.num_layers, self.hidden_size).to(device)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.linear2(x[:, 9::10, :])\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.tdd = nn.Sequential(\n",
    "            nn.Linear(22, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(40)\n",
    "\n",
    "        self.num_layers = 3 # 3, 4\n",
    "        self.hidden_size = 20 # 20\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=40,\n",
    "            hidden_size=self.hidden_size,\n",
    "            dropout=0.5,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(20),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LazyLinear(out_features=4),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)  # the dim corresponds to num_output_classes=4\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x should have shape (N, H, L) = (N, 22, 1000), where\n",
    "            L = sequence length\n",
    "            N = batch size\n",
    "            H = input size\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1) # (N, 1000, 22)\n",
    "        x = self.tdd(x) # (N, 1000, 40)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # device = torch.device('mps')\n",
    "        # h0 = torch.randn(self.num_layers, self.hidden_size).to(device)\n",
    "        # c0 = torch.randn(self.num_layers, self.hidden_size).to(device)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.linear2(x[:, 9::10, :])\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      [128, 4]                  --\n",
       "├─Sequential: 1-1                        [128, 1000, 40]           --\n",
       "│    └─Linear: 2-1                       [128, 1000, 40]           920\n",
       "│    └─ReLU: 2-2                         [128, 1000, 40]           --\n",
       "│    └─Dropout: 2-3                      [128, 1000, 40]           --\n",
       "├─BatchNorm1d: 1-2                       [128, 40, 1000]           80\n",
       "├─LSTM: 1-3                              [128, 1000, 20]           11,680\n",
       "├─Sequential: 1-4                        [128, 4]                  --\n",
       "│    └─Flatten: 2-4                      [128, 2000]               --\n",
       "│    └─Linear: 2-5                       [128, 20]                 40,020\n",
       "│    └─ReLU: 2-6                         [128, 20]                 --\n",
       "│    └─BatchNorm1d: 2-7                  [128, 20]                 40\n",
       "│    └─Dropout: 2-8                      [128, 20]                 --\n",
       "│    └─Linear: 2-9                       [128, 4]                  84\n",
       "├─Softmax: 1-5                           [128, 4]                  --\n",
       "==========================================================================================\n",
       "Total params: 52,824\n",
       "Trainable params: 52,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.50\n",
       "==========================================================================================\n",
       "Input size (MB): 11.26\n",
       "Forward/backward pass size (MB): 102.45\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 113.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 128\n",
    "sample_device = torch.device('mps')\n",
    "sample_model = RNN().to(sample_device)\n",
    "test_input = torch.randn(128, 22, 1000).to(sample_device)\n",
    "print(sample_model(test_input).shape)\n",
    "summary(sample_model, (bsz, 22, 1000), device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting on All Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 70 338 443\n",
      "(8885, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "bsz = 128\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), shuffle=True, batch_size=bsz)\n",
    "train_aug_loader = DataLoader(TensorDataset(torch.from_numpy(X_train_aug), torch.from_numpy(y_train_aug)), shuffle=True, batch_size=bsz)\n",
    "val_loader = DataLoader(TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid)), shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)), shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(train_aug_loader), len(val_loader), len(test_loader))\n",
    "\n",
    "print(X_train_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (tdd): Sequential(\n",
      "    (0): Linear(in_features=22, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (bn): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(40, 20, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (linear2): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): LazyLinear(in_features=0, out_features=20, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): LazyLinear(in_features=0, out_features=4, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "model = RNN().to(device)\n",
    "print(model)\n",
    "num_chans, sequence_length = 22, 1000  # Image dimensions\n",
    "test_input = torch.randn(bsz, num_chans, sequence_length).to(device)\n",
    "print(model(test_input).shape)\n",
    "# pred = model(test_input)[0]\n",
    "# print(nn.CrossEntropyLoss()(pred, torch.from_numpy(y_train[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a7e91d30a3495685a6bbeab1f2c655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.377, Val acc: 0.346\n",
      "Train acc: 0.413, Val acc: 0.382\n",
      "Train acc: 0.444, Val acc: 0.402\n",
      "Train acc: 0.468, Val acc: 0.426\n",
      "Train acc: 0.480, Val acc: 0.432\n",
      "Train acc: 0.497, Val acc: 0.444\n",
      "Train acc: 0.499, Val acc: 0.456\n",
      "Train acc: 0.513, Val acc: 0.453\n",
      "Train acc: 0.513, Val acc: 0.426\n",
      "Train acc: 0.521, Val acc: 0.470\n",
      "Train acc: 0.524, Val acc: 0.453\n",
      "Train acc: 0.528, Val acc: 0.503\n",
      "Train acc: 0.541, Val acc: 0.506\n",
      "Train acc: 0.541, Val acc: 0.476\n",
      "Train acc: 0.548, Val acc: 0.497\n",
      "Train acc: 0.543, Val acc: 0.447\n",
      "Train acc: 0.551, Val acc: 0.470\n",
      "Train acc: 0.541, Val acc: 0.438\n",
      "Train acc: 0.550, Val acc: 0.479\n",
      "Train acc: 0.566, Val acc: 0.470\n",
      "Train acc: 0.566, Val acc: 0.494\n",
      "Train acc: 0.570, Val acc: 0.435\n",
      "Train acc: 0.559, Val acc: 0.447\n",
      "Train acc: 0.566, Val acc: 0.417\n",
      "Train acc: 0.573, Val acc: 0.429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m train_correct_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (train_x, train_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m---> 16\u001b[0m     train_x \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     train_y \u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cel_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000005)\n",
    "num_epochs = 100\n",
    "loader = train_aug_loader\n",
    "\n",
    "best_model = model\n",
    "best_val = 0\n",
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "for epoch_idx in tqdm(range(num_epochs)):\n",
    "    # Set model to train mode - useful for layers such as BatchNorm or Dropout whose behaviors change between train/eval\n",
    "    model.train()\n",
    "    train_count = 0\n",
    "    train_correct_count = 0\n",
    "    for batch_idx, (train_x, train_y) in enumerate(loader):\n",
    "        train_x = train_x.float().to(device)\n",
    "        train_y = train_y.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_x)\n",
    "        loss = cel_loss(logits, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = torch.argmax(logits, dim=-1)\n",
    "            train_correct_count += torch.sum(y_hat == train_y, axis=-1)\n",
    "            train_count += train_x.size(0)\n",
    "\n",
    "    train_acc = train_correct_count / train_count\n",
    "    train_acc_hist.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_count = 0\n",
    "    val_correct_count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (val_x, val_y) in enumerate(val_loader):\n",
    "            val_x = val_x.float().to(device)\n",
    "            val_y = val_y.long().to(device)\n",
    "            logits = model(val_x).detach()\n",
    "            y_hat = torch.argmax(logits, dim=-1)\n",
    "            val_correct_count += torch.sum(y_hat == val_y, axis=-1)\n",
    "            val_count += val_x.size(0)\n",
    "    val_acc = val_correct_count / val_count\n",
    "    val_acc_hist.append(val_acc)\n",
    "    print('Train acc: {:.3f}, Val acc: {:.3f}'.format(train_acc, val_acc))\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        best_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4920993447303772\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "val_count = 0\n",
    "val_correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for idx, (val_x, val_y) in enumerate(test_loader):\n",
    "        val_x = val_x.float().to(device)\n",
    "        val_y = val_y.long().to(device)\n",
    "        logits = best_model(val_x).detach()\n",
    "        y_hat = torch.argmax(logits, dim=-1)\n",
    "        val_correct_count += torch.sum(y_hat == val_y, axis=-1)\n",
    "        val_count += val_x.size(0)\n",
    "val_acc = val_correct_count / val_count\n",
    "\n",
    "print(\"Test Accuracy:\", val_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5059, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting on Subject 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 70 38 50\n",
      "(8885, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "bsz = 128\n",
    "\n",
    "person_train = person_train_valid[:1777]\n",
    "person_valid = person_train_valid[1777:]\n",
    "\n",
    "train_index = np.where(person_train==0)[0]\n",
    "train_aug_index = np.where(person_train_aug==0)[0]\n",
    "val_index = np.where(person_valid==0)[0]\n",
    "test_index = np.where(person_test==0)[0]\n",
    "\n",
    "X_train_sub = X_train[train_index]\n",
    "y_train_sub = y_train[train_index]\n",
    "X_train_aug_sub = X_train_aug[train_aug_index]\n",
    "y_train_aug_sub = y_train_aug[train_aug_index]\n",
    "X_val_sub = X_valid[val_index]\n",
    "y_val_sub = y_valid[val_index]\n",
    "X_test_sub = X_test[test_index]\n",
    "y_test_sub = y_test[test_index]\n",
    "\n",
    "train_sub_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), shuffle=True, batch_size=bsz)\n",
    "train_sub_aug_loader = DataLoader(TensorDataset(torch.from_numpy(X_train_aug), torch.from_numpy(y_train_aug)), shuffle=True, batch_size=bsz)\n",
    "val_sub_loader = DataLoader(TensorDataset(torch.from_numpy(X_val_sub), torch.from_numpy(y_val_sub)), shuffle=False)\n",
    "test_sub_loader = DataLoader(TensorDataset(torch.from_numpy(X_test_sub), torch.from_numpy(y_test_sub)), shuffle=False)\n",
    "\n",
    "print(len(train_sub_loader), len(train_sub_aug_loader), len(val_sub_loader), len(test_sub_loader))\n",
    "\n",
    "print(X_train_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yubo/miniforge3/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (tdd): Sequential(\n",
      "    (0): Linear(in_features=22, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (bn): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(40, 20, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (linear2): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): LazyLinear(in_features=0, out_features=20, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): LazyLinear(in_features=0, out_features=4, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "model = RNN().to(device)\n",
    "print(model)\n",
    "num_chans, sequence_length = 22, 1000  # Image dimensions\n",
    "test_input = torch.randn(bsz, num_chans, sequence_length).to(device)\n",
    "print(model(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb47909b314e2f860a9a290e207f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.291, Val acc: 0.447\n",
      "Train acc: 0.379, Val acc: 0.526\n",
      "Train acc: 0.410, Val acc: 0.632\n",
      "Train acc: 0.432, Val acc: 0.500\n",
      "Train acc: 0.445, Val acc: 0.421\n",
      "Train acc: 0.464, Val acc: 0.500\n",
      "Train acc: 0.467, Val acc: 0.553\n",
      "Train acc: 0.475, Val acc: 0.553\n",
      "Train acc: 0.486, Val acc: 0.447\n",
      "Train acc: 0.497, Val acc: 0.474\n",
      "Train acc: 0.500, Val acc: 0.553\n",
      "Train acc: 0.512, Val acc: 0.632\n",
      "Train acc: 0.505, Val acc: 0.421\n",
      "Train acc: 0.516, Val acc: 0.553\n",
      "Train acc: 0.525, Val acc: 0.447\n",
      "Train acc: 0.521, Val acc: 0.526\n",
      "Train acc: 0.521, Val acc: 0.500\n",
      "Train acc: 0.532, Val acc: 0.500\n",
      "Train acc: 0.542, Val acc: 0.526\n",
      "Train acc: 0.542, Val acc: 0.579\n",
      "Train acc: 0.542, Val acc: 0.316\n",
      "Train acc: 0.552, Val acc: 0.579\n",
      "Train acc: 0.559, Val acc: 0.474\n",
      "Train acc: 0.554, Val acc: 0.474\n",
      "Train acc: 0.557, Val acc: 0.500\n",
      "Train acc: 0.558, Val acc: 0.474\n",
      "Train acc: 0.559, Val acc: 0.526\n",
      "Train acc: 0.554, Val acc: 0.395\n",
      "Train acc: 0.559, Val acc: 0.395\n",
      "Train acc: 0.567, Val acc: 0.553\n",
      "Train acc: 0.566, Val acc: 0.553\n",
      "Train acc: 0.565, Val acc: 0.526\n",
      "Train acc: 0.579, Val acc: 0.395\n",
      "Train acc: 0.576, Val acc: 0.553\n",
      "Train acc: 0.578, Val acc: 0.447\n",
      "Train acc: 0.582, Val acc: 0.500\n",
      "Train acc: 0.586, Val acc: 0.500\n",
      "Train acc: 0.587, Val acc: 0.447\n",
      "Train acc: 0.581, Val acc: 0.474\n",
      "Train acc: 0.592, Val acc: 0.421\n",
      "Train acc: 0.580, Val acc: 0.500\n",
      "Train acc: 0.594, Val acc: 0.447\n",
      "Train acc: 0.588, Val acc: 0.395\n",
      "Train acc: 0.594, Val acc: 0.474\n",
      "Train acc: 0.593, Val acc: 0.526\n",
      "Train acc: 0.594, Val acc: 0.421\n",
      "Train acc: 0.584, Val acc: 0.474\n",
      "Train acc: 0.587, Val acc: 0.579\n",
      "Train acc: 0.595, Val acc: 0.447\n",
      "Train acc: 0.593, Val acc: 0.447\n",
      "Train acc: 0.597, Val acc: 0.500\n",
      "Train acc: 0.604, Val acc: 0.500\n",
      "Train acc: 0.599, Val acc: 0.395\n",
      "Train acc: 0.603, Val acc: 0.500\n",
      "Train acc: 0.607, Val acc: 0.526\n",
      "Train acc: 0.605, Val acc: 0.447\n",
      "Train acc: 0.597, Val acc: 0.474\n",
      "Train acc: 0.602, Val acc: 0.526\n",
      "Train acc: 0.605, Val acc: 0.447\n",
      "Train acc: 0.604, Val acc: 0.474\n",
      "Train acc: 0.612, Val acc: 0.500\n",
      "Train acc: 0.610, Val acc: 0.421\n",
      "Train acc: 0.610, Val acc: 0.447\n",
      "Train acc: 0.612, Val acc: 0.421\n",
      "Train acc: 0.611, Val acc: 0.447\n",
      "Train acc: 0.618, Val acc: 0.474\n",
      "Train acc: 0.615, Val acc: 0.553\n",
      "Train acc: 0.623, Val acc: 0.395\n",
      "Train acc: 0.616, Val acc: 0.447\n",
      "Train acc: 0.618, Val acc: 0.421\n",
      "Train acc: 0.619, Val acc: 0.526\n",
      "Train acc: 0.616, Val acc: 0.579\n",
      "Train acc: 0.618, Val acc: 0.500\n",
      "Train acc: 0.615, Val acc: 0.474\n",
      "Train acc: 0.616, Val acc: 0.474\n",
      "Train acc: 0.614, Val acc: 0.447\n",
      "Train acc: 0.599, Val acc: 0.447\n",
      "Train acc: 0.620, Val acc: 0.447\n",
      "Train acc: 0.609, Val acc: 0.474\n",
      "Train acc: 0.616, Val acc: 0.553\n",
      "Train acc: 0.612, Val acc: 0.526\n",
      "Train acc: 0.590, Val acc: 0.500\n",
      "Train acc: 0.616, Val acc: 0.421\n",
      "Train acc: 0.617, Val acc: 0.474\n",
      "Train acc: 0.624, Val acc: 0.421\n",
      "Train acc: 0.618, Val acc: 0.421\n",
      "Train acc: 0.602, Val acc: 0.579\n",
      "Train acc: 0.607, Val acc: 0.526\n",
      "Train acc: 0.588, Val acc: 0.526\n",
      "Train acc: 0.607, Val acc: 0.421\n",
      "Train acc: 0.616, Val acc: 0.500\n",
      "Train acc: 0.609, Val acc: 0.526\n",
      "Train acc: 0.623, Val acc: 0.474\n",
      "Train acc: 0.618, Val acc: 0.500\n",
      "Train acc: 0.619, Val acc: 0.474\n",
      "Train acc: 0.627, Val acc: 0.447\n",
      "Train acc: 0.616, Val acc: 0.447\n",
      "Train acc: 0.619, Val acc: 0.474\n",
      "Train acc: 0.629, Val acc: 0.553\n",
      "Train acc: 0.637, Val acc: 0.447\n"
     ]
    }
   ],
   "source": [
    "cel_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000005)\n",
    "num_epochs = 100\n",
    "loader = train_sub_aug_loader\n",
    "\n",
    "best_sub_model = model\n",
    "best_sub_val = 0\n",
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "for epoch_idx in tqdm(range(num_epochs)):\n",
    "    # Set model to train mode - useful for layers such as BatchNorm or Dropout whose behaviors change between train/eval\n",
    "    model.train()\n",
    "    train_count = 0\n",
    "    train_correct_count = 0\n",
    "    for batch_idx, (train_x, train_y) in enumerate(loader):\n",
    "        train_x = train_x.float().to(device)\n",
    "        train_y = train_y.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_x)\n",
    "        loss = cel_loss(logits, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = torch.argmax(logits, dim=-1)\n",
    "            train_correct_count += torch.sum(y_hat == train_y, axis=-1)\n",
    "            train_count += train_x.size(0)\n",
    "\n",
    "    train_acc = train_correct_count / train_count\n",
    "    train_acc_hist.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_count = 0\n",
    "    val_correct_count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (val_x, val_y) in enumerate(val_sub_loader):\n",
    "            val_x = val_x.float().to(device)\n",
    "            val_y = val_y.long().to(device)\n",
    "            logits = model(val_x).detach()\n",
    "            y_hat = torch.argmax(logits, dim=-1)\n",
    "            val_correct_count += torch.sum(y_hat == val_y, axis=-1)\n",
    "            val_count += val_x.size(0)\n",
    "    val_acc = val_correct_count / val_count\n",
    "    val_acc_hist.append(val_acc)\n",
    "    print('Train acc: {:.3f}, Val acc: {:.3f}'.format(train_acc, val_acc))\n",
    "\n",
    "    if val_acc > best_sub_val:\n",
    "        best_sub_val = val_acc\n",
    "        best_sub_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_sub_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbest_sub_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m val_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m val_correct_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_sub_model' is not defined"
     ]
    }
   ],
   "source": [
    "best_sub_model.eval()\n",
    "val_count = 0\n",
    "val_correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for idx, (val_x, val_y) in enumerate(test_sub_loader):\n",
    "        val_x = val_x.float().to(device)\n",
    "        val_y = val_y.long().to(device)\n",
    "        logits = best_sub_model(val_x).detach()\n",
    "        y_hat = torch.argmax(logits, dim=-1)\n",
    "        val_correct_count += torch.sum(y_hat == val_y, axis=-1)\n",
    "        val_count += val_x.size(0)\n",
    "val_acc = val_correct_count / val_count\n",
    "\n",
    "print(\"Test Accuracy:\", val_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6316, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(best_sub_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
